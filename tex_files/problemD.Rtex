<<echo=FALSE, cache=FALSE>>=
read_chunk("../code/functions.R")
@
<<echo=FALSE, cache=FALSE>>=
read_chunk("../code/problemD.R")
@
<<echo=FALSE>>=
source("../code/functions.R")
library(ggplot2)
library(tibble)
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We consider a vector $\bm{y} = (y_1, y_2, y_3, y_4)$ of multinomially distributed counts with probabilities $\bm{p} = (\frac{1}{2} + \frac{\theta}{4}, \frac{1-\theta}{4}, \frac{1-\theta}{4}, \frac{\theta}{4})$. The multinomial mass function is given by $f(y|\theta) \propto (2+\theta)^{y_1} (1-\theta)^{y_2+y_3} \theta^{y_4}$. The observed data is $y = (125, 18, 20, 34)$. First we assume a uniform prior on $(0, 1)$, resulting in a posterior density for $\theta$
%
\begin{equation}
\label{eq:f_theta}
    f(\theta|\bm{y}) \propto (2+\theta)^{y_1} (1-\theta)^{y_2+y_3} \theta^{y_4} \eqdef f^*(\theta | y) \, , \quad \theta \in (0,1) \, .
\end{equation}
%
In this task we are mainly interested in the posterior mean $\E(\theta|\bm{y})$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{1.}
We wish to construct a rejection sampling algorithm to simulate from $f(\theta|\bm{y})$ using $\mathrm{U}(0,1)$ as the proposal density. This can be done exactly or approximately. To do it exactly we need to find $\max_{\theta} f^*(\theta)$. For the observed $\bm{y}$ we get
%
\begin{equation*}
    \od{}{\theta} f^*(\theta|y) \propto -197 \theta^2 + 15 \theta + 68 = 0 \quad \Rightarrow \quad \theta_m \approx 0.62682150 \, ,
\end{equation*}
%
and $f^*(\theta_m) \approx 1.83884 \cdot 10^{29}$. In the implementation the maximum is found numerically. We also need $c = \int_0^1 f^*(\theta|y) \dif \theta \approx 2.3577 \cdot 10^{28}$, which is found numerically. This gives the ideal overall acceptance probability $\frac{1}{k} = \max_{\theta} f(\theta|\bm{y}) = f^*(\theta_m) / c \approx 7.780$ since the proposal density is $g(x) = 1$.

To implement the approximate algorithm described in lecture 4 $k$ and $c$ are not needed, since
%
\begin{equation*}
    w_i = \frac{f(x_i)}{g(x_i)} \bigg/ \sum_{j=1}^n\frac{f(x_j)}{g(x_j)} = \frac{f^*(x_i)}{g(x_i)} \bigg/ \sum_{j=1}^n\frac{f^*(x_j)}{g(x_j)}
\end{equation*}
%
Both methods are implemented in the code shown below.
%
<<f_star_d, eval=FALSE>>=
@
\vspace{-1em}
<<r_d, eval=FALSE>>=
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{2.}
In the following code we sample 10000 times from $f(\theta|\bm{y})$ using both the exact and the approximate sampling method, estimate the mean and compare with the theoretical distribution and mean.
%
<<D2, fig.align='center', fig.width=6, fig.height=4, fig.cap="The red line is the theoretical posterior density $f(\\theta|y)$ (see \\eqref{eq:f_theta}). The bins are frequency histograms of simulations from an exact simulation algorithm (light blue) and an approximate algorithm (light grey). The colors are mixed where they overlap. The vertical red line is the theoretical mean, and the dark green line is the empirical mean from the exact algorithm.">>=
@
%
The estimated means from the exact method and the approximate method are \Sexpr{signif(empirical_mean, 4)} and \Sexpr{signif(empirical_mean_approx, 4)}, respectively, close to the theoretical (numerically computed) mean $\mu =$ \Sexpr{signif(true_mean, 4)}. The exact algorithm gives samples matching the theoretical density especially well. The approximate density has a larger spread than the theoretical density (this becomes evident when running more simulations, e.g. 100000). This is not surprising given the uniform prior. The estimates are more accurate when increasing m (the number of samples we resample from), which comes at a high computational cost. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{3.}
In the code below we check how many random numbers the algorithms have generated.
%
<<D3>>=
@
%
The exact algorithm generates on average \Sexpr{n_random_numbers} $/$ \Sexpr{n} $=$ \Sexpr{n_random_numbers/n} random numbers to obtain one sample of $f(\theta|\bm{y})$, closely matching the inverse of the acceptance probability calculated approximately before. The approximate algorithm generates $m$, set to 20 in the code, random numbers per sample. With this $m$ the approximation is easily distinguished from the true distribution. The comparison might have had a different conclusion for a different proposal density, but here the exact algorithm is both more efficient and more notably precise.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{4.}
We now assume a Beta$(1,5)$ prior
%
\begin{equation*}
    f_5(\theta) = \frac{\theta^{\alpha-1}(1-\theta)^{\beta-1}}{B(\alpha, \beta)} = \frac{(1-\theta)^4}{B(1,5)} \propto (1-\theta)^4 \, .
\end{equation*}
%
The new posterior is given by
%
\begin{equation*}
    f_5(\theta|\bm{y}) \propto f(\bm{y}|\theta)f_5(\theta) \propto (2+\theta)^{y_1} (1-\theta)^{y_2+y_3+4} \theta^{y_4} \eqdef f_5^*(\theta|\bm{y}) \, , \quad \theta \in (0,1) \, .
\end{equation*}
%
We wish to use importance sampling to estimate the new posterior mean $\mu$. The algorithm is as following: Sample $n$ times from a distribution $g(x)$ where $g(x) > 0$ where $f_5(\theta|\bm{y}) > 0$. We use $n = 10000$ and $g(x) = f(\theta|\bm{y})$. Let $w(x_i) = f_5(x_i|\bm{y}) / f(x_i|\bm{y})$. Then $\hat{\mu}_{IS} = \frac{\sum_i x_i w(x_i)}{n}$ is an unbiased estimate of $\mu$. This estimator, however, requires knowledge of the integration constant $c = \int_0^1 f_5^*(x|\bm{y}) \dif x$. We could calculate this numerically, but we choose to proceed with a self-normalizing (but in general not unbiased) estimate of $\mu$
%
\begin{equation*}
    \tilde{\mu}_{IS} = \frac{\sum_{i=1}^n x_i w(x_i)}{\sum_{i=1}^n w(x_i)} = \frac{\sum_{i=1}^n x_i w^*(x_i)}{\sum_{i=1}^n w^*(x_i)} \, , \quad \text{where} \quad w^*(x_i) = \frac{f_5^*(x_i|\bm{y})}{f^*(x_i)} = (1-x_i)^4 \, .
\end{equation*}
%
The method is implemented in the code below.
<<D4>>=
@


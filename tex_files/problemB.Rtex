<<echo=FALSE, cache=FALSE>>=
read_chunk("../code/functions.R")
@
<<echo=FALSE, cache=FALSE>>=
read_chunk("../code/problemB1.R")
@
<<echo=FALSE, cache=FALSE>>=
read_chunk("../code/problemB2.R")
@
<<echo=FALSE, cache=FALSE>>=
read_chunk("../code/problemB3.R")
@
<<echo=FALSE>>=
source("../code/functions.R")
library(ggplot2)
library(tibble)
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{1.}
We consider a Gamma distribution with parameters $\alpha \in (0, 1)$ and $\beta = 1$, i.e.
%
\begin{equation}
\label{eq:f}
    f(x) = 
    \begin{cases}
    \frac{1}{\G(\alpha)} x^{\alpha-1} \e^{-x}, & 0 < x, \\
    0, & \text{otherwise}.
    \end{cases}
\end{equation}
%

\paragraph{(a)}
We wish to generate samples from the distribution given in equation~\eqref{eq:f} using rejection sampling with $g(x)$ (see equation~\eqref{eq:g}) as proposal density. We need to find $k \geq 1$ s.t. $f(x) \leq k \cdot g(x)$. In addition to the trivial case $x \leq 0$ (where $f(x) = g(x) = 0$, there are two cases:
%
\begin{align*}
    0 < x < 1: & \quad \frac{\e^{-x}}{\Gamma(\alpha)c} \leq \frac{1}{\Gamma(\alpha)c} = \frac{\e + \alpha}{\Gamma(\alpha + 1)e} \leq k \\
    x \geq 1: & \quad \frac{x^{\alpha - 1}}{\Gamma(\alpha)c} \leq \frac{1}{\Gamma(\alpha)c} = \frac{\e + \alpha}{\Gamma(\alpha + 1)e} \leq k \, ,
\end{align*}
%
where $c = \e\alpha/(\e + \alpha)$, as before. We set $k$ as small as possible to maximize the overall acceptance probability $k^{-1}$, i.e. $k = [\Gamma(\alpha)c]^{-1}$.

\paragraph{(b)}
The function \texttt{r\_gamma1()} below generates a vector of $n$ independent samples from $f$ with $\alpha < 1$ and $\beta = 1$.
%
<<k, eval=FALSE>>=
@
\vspace{-1em}
<<f_gamma1, eval=FALSE>>=
@
\vspace{-1em}
<<r_gamma1, eval=FALSE>>=
@

Next we test \texttt{r\_gamma1()}.
%
<<B1, fig.align='center', fig.width=6, fig.height=4, fig.cap="Theoretical density (red line) from a Gamma distribution with parameters $\\alpha = 0.5$ and $\\beta = 1$ and frequency histogram of simulations.">>==
@
%
We see in figure~\ref{fig:B1} that the density from the simulations matches $f$ almost perfectly. Also, with $\alpha =$ \Sexpr{alpha}, the empirical mean and variance are \Sexpr{format(round(empirical_mean, 4), nsmall=4)} and \Sexpr{format(round(empirical_var, 4), nsmall=4)}, respectively, which is close to the theoretical value $\alpha$. Simulating with 9 different values of $\alpha$ the greatest relative error of the empirical means and variances lie within a few percent.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{2.}
We now consider a Gamma distribution with parameters $\alpha > 0$ and $\beta = 1$ \eqref{eq:f}. We wish to generate samples from the distribution using the ratio of uniforms method. That is, we sample uniformly from
%
\begin{equation*}
    C_f = \left\{ (x_1, x_2): 0 \leq x_1 \leq \sqrt{f^*\left(\frac{x1}{x2}\right)} \right\} \quad \text{where} \quad f^*(x) =
    \begin{cases}
    x^{\alpha-1} \e^{-x}, & 0 < x\\
    0, & \text{otherwise}.
    \end{cases}
\end{equation*}
%
We know that $C_f \subset [0,a] \times [b_-,b_+]$, where
%
\begin{equation*}
\begin{gathered}
    a = \sqrt{\sup_x f^*(x)} = \left(\frac{\alpha-1}{\e}\right)^\frac{\alpha-1}{2} \, , \quad b_- = - \sqrt{\sup_{x \leq 0} x^2 f^*(x)} = 0 \ \text{and} \\
    b_+ = \sqrt{\sup_{x \geq 0} x^2 f^*(x)} = \left(\frac{\alpha+1}{\e}\right)^\frac{\alpha+1}{2} \, .
\end{gathered}
\end{equation*}
%
This can be utilized by sampling uniformly on $[0,a] \times [b_-,b_+]$ and accepting only values $(x_1, x_2) \in C_f$. Then $y = x_1/x_2$ will be a sample from the desired distribution $f$. 

The problem is that we will get very big numerical values in intermediate steps for large $\alpha$, so we have to be careful and implement a logarithmic version. We let $(y_1,y_2) = (\log x_1, \log x_2)$ and sample from $y_1$ and $y_2$ by letting $u_1$ and $u_2$ be uniform on $(0, 1)$, so that
%
\begin{equation*}
\begin{gathered}
    y_1 = \log{a} + \log{u_1} = \frac{\alpha-1}{2} \log \frac{\alpha-1}{\e} + \log u_1 \quad \text{and} \\
    y_1 = \log{b_+} + \log{u_2} = \frac{\alpha+1}{2} \log \frac{\alpha+1}{\e} + \log u_2 
\end{gathered}
\end{equation*}
%
are distributed as desired, i.e. such that $(x_1, x_2) = (\exp y_1, \exp y_2)$ are distributed uniformly on $[0,a] \times [b_-,b_+]$. Now we accept $(x_1,x_2)$ for which 
%
\begin{equation*}
    \log x_1 \leq 1/2 \cdot \log f^*(x_2/x_1) = \frac{1}{2} \left[(\alpha-1)(\log x_2 - \log x_1) - \frac{x_2}{x_1}\right] \, .
\end{equation*}
%
The algorithm is implemented in the following functions.
%
<<log_sqrt_f_star, eval=FALSE>>=
@
\vspace{-1em}
<<r_gamma2, eval=FALSE>>=
@

Next we test \texttt{r\_gamma2()}.
<<B2, eval=TRUE, fig.align='center', fig.width=6, fig.height=4, fig.cap=c("Theoretical density (red line) from a Gamma distribution with parameters $\\alpha = 5$ and $\\beta = 1$ and frequency histogram of simulations.", "Number of tries needed to generate 1000 realizations from a Gamma distribution with $\\beta = 1$ as a function of alpha.", "caption", "caption")>>=
@
%
We see in figure~\ref{fig:B21} that the density from the simulations matches $f$ almost perfectly. Also, with $\alpha =$ \Sexpr{alpha}, the empirical mean and variance are \Sexpr{format(round(empirical_mean, 4), nsmall=4)} and \Sexpr{format(round(empirical_var, 4), nsmall=4)}, respectively, which is close to the theoretical value $\alpha$. Simulating with 4 different values of $\alpha$, including two large ones to test for numerical stability, the greatest relative error of the empirical means and variances lie within a few percent.

Figure~\ref{fig:B22} shows a plot of the number of tries needed to simulate 1000 realizations for different $\alpha \in (1, 2000]$. The number of tries increases rapidly for small increasing $\alpha$. This indicates the relative size of $C_f$ compared to $[0,a] \times [b_-,b_+]$ decreases rapidly. The second derivative of the number of tries needed is negative though, i.e. the number flattens out while $alpha$ increases.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{3.}
The parameter $\beta$ in a Gamma distribution
%
\begin{equation*}
    f(x) = 
    \begin{cases}
    \frac{\beta^\alpha}{\G(\alpha)} x^{\alpha-1} \e^{-\beta x}, & 0 < x, \\
    0, & \text{otherwise}, 
    \end{cases}
\end{equation*}
%
$\alpha, \beta > 1$, is an inverse scale parameter. Thus, if $X \sim \text{Gamma}(\alpha, 1)$, then $X / \beta \sim \text{Gamma}(\alpha, \beta)$. Furthermore $\text{Gamma}(1,1) = \text{Exp}(1)$. So we can simulate from an arbitrary Gamma distribution using the following function.
%
<<r_gamma, eval=FALSE>>=
@

The following code illustrates that it works as expected.
%
<<B3, eval=TRUE, fig.align='center', fig.width=6, fig.height=4, fig.cap="Theoretical density (red line) from a Gamma distribution with parameters $\\alpha = 10$ and $\\beta = 10$ and frequency histogram of simulations.">>=
@

Figure~\ref{fig:B3} shows the density from the simulations matches $f$ almost perfectly. With $\alpha =$ \Sexpr{alpha} and $\beta =$ \Sexpr{beta}, the empirical mean and variance are \Sexpr{format(round(empirical_mean, 4), nsmall=4)} and \Sexpr{format(round(empirical_var, 4), nsmall=4)}, respectively, which is close to the theoretical values $\alpha / \beta =$ \Sexpr{alpha/beta} and $\alpha / \beta^2 =$ \Sexpr{alpha/beta^2}, respectively. Simulating with 25 different combinations of values of $\alpha$ and $\beta$, the greatest relative error of the empirical means and variances lie within a few percent.


